<quiz>
  <metadata>
    <title>Text Classification and Sentiment Analysis</title>
    <open_time>2012-03-24 0001</open_time>
    <soft_close_time>2012-04-10 2359</soft_close_time>
    <hard_close_time>2012-05-23 2359</hard_close_time>
    <duration>0</duration>
    <retry_delay>10</retry_delay>
    <maximum_submissions>5</maximum_submissions>
    <modified_time>1337741966052</modified_time>
    <parameters>
      <show_explanations>
        <question>after_hard_close_time</question>
        <option>before_soft_close_time</option>
        <score>before_soft_close_time</score>
      </show_explanations>
    </parameters>
    <maximum_score>4</maximum_score>
  </metadata>
  <preamble><![CDATA[]]></preamble>
  <data>
    <question_groups>
      <question_group select="1">
        <preamble><![CDATA[]]></preamble>
        <question id="ee7461fca29652c5160f875e7dfdcfe2" type="GS_Choice_Answer_Question">
          <metadata>
            <parameters>
              <rescale_score>1</rescale_score>
              <choice_type>radio</choice_type>
            </parameters>
          </metadata>
          <data>
            <text><![CDATA[<p>Assume the following probabilities for each word being part of a <em>positive</em> or <em>negative</em> movie review.</p>
<table>
<tr><td> </td><td><b>pos</b></td><td><b>neg</b></td></tr>
<tr><td><em>I</em></td><td>0.09</td><td>0.16</td></tr>
<tr><td><em>always</em></td><td>0.07</td><td>0.06</td></tr>
<tr><td><em>like</em></td><td>0.29</td><td>0.06</td></tr>
<tr><td><em>foreign</em></td><td>0.04</td><td>0.15</td></tr>
<tr><td><em>films</em></td><td>0.08</td><td>0.11</td></tr>
</table>

<p>Now consider the sentence...
<pre>I always like foreign films.</pre>
<p>Using Naïve Bayes and assuming equal prior probability for each class, will we classify this sentence as being part of a <em>positive</em> or <em>negative</em> review?]]></text>
            <explanation><![CDATA[With Naïve Bayes as a language model, we're looking for the class that maximizes the probability of the sentence. (See lecture slide #41 in the slide set on Text Classification and Naïve Bayes.) Because we're looking for the argmax, we can ignore the denominator (the likelihood of the sentence, in general), and the question states that the classes have equal prior probability, so we simply find the product of the likelihoods of each word, i.e., multiply each column and take the larger.]]></explanation>
            <option_groups randomize="true">
              <option_group select="all">
                <option id="ba126b60767fc0afa50c65746dbfb1c9" selected_score="0" unselected_score="0">
                  <text><![CDATA[pos]]></text>
                  <explanation><![CDATA[]]></explanation>
                </option>
                <option id="d29d3fae07eeb19822dfb5e63bac1064" selected_score="1" unselected_score="0">
                  <text><![CDATA[neg]]></text>
                  <explanation><![CDATA[]]></explanation>
                </option>
              </option_group>
            </option_groups>
          </data>
        </question>
        <question id="09485f335f62d944f2c09036ba973d67" type="GS_Choice_Answer_Question">
          <metadata>
            <parameters>
              <rescale_score>1</rescale_score>
              <choice_type>radio</choice_type>
            </parameters>
          </metadata>
          <data>
            <text><![CDATA[<p>Assume the following probabilities for each word being part of a <em>positive</em> or <em>negative</em> movie review.</p>
<table>
<tr><td> </td><td><b>pos</b></td><td><b>neg</b></td></tr>
<tr><td><em>I</em></td><td>0.09</td><td>0.18</td></tr>
<tr><td><em>always</em></td><td>0.09</td><td>0.05</td></tr>
<tr><td><em>like</em></td><td>0.19</td><td>0.04</td></tr>
<tr><td><em>foreign</em></td><td>0.04</td><td>0.11</td></tr>
<tr><td><em>films</em></td><td>0.09</td><td>0.12</td></tr>
</table>

<p>Now consider the sentence...
<pre>I always like foreign films.</pre>
<p>Using Naïve Bayes and assuming equal prior probability for each class, will we classify this sentence as being part of a <em>positive</em> or <em>negative</em> review?]]></text>
            <explanation><![CDATA[With Naïve Bayes as a language model, we're looking for the class that maximizes the probability of the sentence. (See lecture slide #41 in the slide set on Text Classification and Naïve Bayes.) Because we're looking for the argmax, we can ignore the denominator (the likelihood of the sentence, in general), and the question states that the classes have equal prior probability, so we simply find the product of the likelihoods of each word, i.e., multiply each column and take the larger.]]></explanation>
            <option_groups randomize="true">
              <option_group select="all">
                <option id="000fa845ea842053b13038aab2d0b528" selected_score="1" unselected_score="0">
                  <text><![CDATA[pos]]></text>
                  <explanation><![CDATA[]]></explanation>
                </option>
                <option id="a8e4ad605a27d5b78d0fb3466304fe46" selected_score="0" unselected_score="0">
                  <text><![CDATA[neg]]></text>
                  <explanation><![CDATA[]]></explanation>
                </option>
              </option_group>
            </option_groups>
          </data>
        </question>
        <question id="333702857bda0e8864193110b5dac67c" type="GS_Choice_Answer_Question">
          <metadata>
            <parameters>
              <rescale_score>1</rescale_score>
              <choice_type>radio</choice_type>
            </parameters>
          </metadata>
          <data>
            <text><![CDATA[<p>Assume the following probabilities for each word being part of a <em>positive</em> or <em>negative</em> movie review.</p>
<table>
<tr><td> </td><td><b>pos</b></td><td><b>neg</b></td></tr>
<tr><td><em>I</em></td><td>0.13</td><td>0.29</td></tr>
<tr><td><em>always</em></td><td>0.08</td><td>0.06</td></tr>
<tr><td><em>like</em></td><td>0.19</td><td>0.03</td></tr>
<tr><td><em>foreign</em></td><td>0.07</td><td>0.21</td></tr>
<tr><td><em>films</em></td><td>0.11</td><td>0.14</td></tr>
</table>

<p>Now consider the sentence...
<pre>I always like foreign films.</pre>
<p>Using Naïve Bayes and assuming equal prior probability for each class, will we classify this sentence as being part of a <em>positive</em> or <em>negative</em> review?]]></text>
            <explanation><![CDATA[With Naïve Bayes as a language model, we're looking for the class that maximizes the probability of the sentence. (See lecture slide #41 in the slide set on Text Classification and Naïve Bayes.) Because we're looking for the argmax, we can ignore the denominator (the likelihood of the sentence, in general), and the question states that the classes have equal prior probability, so we simply find the product of the likelihoods of each word, i.e., multiply each column and take the larger.]]></explanation>
            <option_groups randomize="true">
              <option_group select="all">
                <option id="b2e0581966a40904dce7e174cc982de9" selected_score="0" unselected_score="0">
                  <text><![CDATA[pos]]></text>
                  <explanation><![CDATA[]]></explanation>
                </option>
                <option id="82bb1ea4a42f1d0c1cec28d588416723" selected_score="1" unselected_score="0">
                  <text><![CDATA[neg]]></text>
                  <explanation><![CDATA[]]></explanation>
                </option>
              </option_group>
            </option_groups>
          </data>
        </question>
      </question_group>
      <question_group select="1">
        <preamble><![CDATA[]]></preamble>
        <question id="1461ffb2467828b486e50f181d416635" type="GS_Choice_Answer_Question">
          <metadata>
            <parameters>
              <rescale_score>1</rescale_score>
              <choice_type>radio</choice_type>
            </parameters>
          </metadata>
          <data>
            <text><![CDATA[<p>Suppose we have the following short movie reviews, each labeled with a genre, either <em>comedy</em> or <em>action</em>.</p>
<table>
<tr><td>1.</td><td>fun, couple, love, love</td><td><b>comedy</b></td></tr>
<tr><td>2.</td><td>fast, furious, shoot</td><td><b>action</b></td></tr>
<tr><td>3.</td><td>couple, fly, fast, fun, fun</td><td><b>comedy</b></td></tr>
<tr><td>4.</td><td>furious, shoot, shoot, fun</td><td><b>action</b></td></tr>
<tr><td>5.</td><td>fly, fast, shoot, love</td><td><b>action</b></td></tr>
</table>
<p>Now consider the following new review...</p>
<pre>
fast, couple, shoot, fly
</pre>
Using a simple $$Naïve$$ $$Bayes$$ approach with $$Laplace$$ $$Smoothing$$, how would we classify this, <em>comedy</em> or <em>action</em>? <b><em>(Hint: Don't forget to include the prior.)]]></text>
            <explanation><![CDATA[(See lecture slide #44 in the slide set on Text Classification and Naïve Bayes.) If <em>r</em> is our new review, we're looking for the class (genre) that maximizes (argmax) <em>P(c|r)</em>, where <em>P(c|r)</em> is proportional to the prior probability of the class, <em>P(c)</em>, times the product of <em>P(w|c)</em> for each word in <em>r</em>. <em>P(c)</em> is the number of reviews for each genre, divided by the total number of reviews. <em>P(w|c)</em> is the count of word <em>w</em> in class <em>c</em> plus 1 (i.e., smoothed), divided by the total count for <em>w</em> plus the vocabulary size (the number of unique words).]]></explanation>
            <option_groups randomize="true">
              <option_group select="all">
                <option id="d1176eb077773d390c27d02ede9c128a" selected_score="0" unselected_score="0">
                  <text><![CDATA[comedy]]></text>
                  <explanation><![CDATA[]]></explanation>
                </option>
                <option id="1fd12aa962c43c1d1792d15423761880" selected_score="1" unselected_score="0">
                  <text><![CDATA[action]]></text>
                  <explanation><![CDATA[]]></explanation>
                </option>
              </option_group>
            </option_groups>
          </data>
        </question>
        <question id="22c018825c89bf4e4de03c195bdfc693" type="GS_Choice_Answer_Question">
          <metadata>
            <parameters>
              <rescale_score>1</rescale_score>
              <choice_type>radio</choice_type>
            </parameters>
          </metadata>
          <data>
            <text><![CDATA[<p>Suppose we have the following short movie reviews, each labeled with a genre, either <em>comedy</em> or <em>action</em>.</p>
<table>
<tr><td>1.</td><td>fun, couple, love, love</td><td><b>comedy</b></td></tr>
<tr><td>2.</td><td>fast, furious, shoot</td><td><b>action</b></td></tr>
<tr><td>3.</td><td>couple, fly, fast, fun, fun</td><td><b>comedy</b></td></tr>
<tr><td>4.</td><td>furious, shoot, shoot, fun</td><td><b>action</b></td></tr>
<tr><td>5.</td><td>fly, fast, shoot, love</td><td><b>action</b></td></tr>
</table>
<p>Now consider the following new review...</p>
<pre>
fun, couple, shoot, fast
</pre>
Using a simple $$Naïve$$ $$Bayes$$ approach with $$Laplace$$ $$Smoothing$$, how would we classify this, <em>comedy</em> or <em>action</em>? <b><em>(Hint: Don't forget to include the prior.)]]></text>
            <explanation><![CDATA[(See lecture slide #44 in the slide set on Text Classification and Naïve Bayes.) If <em>r</em> is our new review, we're looking for the class (genre) that maximizes (argmax) <em>P(c|r)</em>, where <em>P(c|r)</em> is proportional to the prior probability of the class, <em>P(c)</em>, times the product of <em>P(w|c)</em> for each word in <em>r</em>. <em>P(c)</em> is the number of reviews for each genre, divided by the total number of reviews. <em>P(w|c)</em> is the count of word <em>w</em> in class <em>c</em> plus 1 (i.e., smoothed), divided by the total count for <em>w</em> plus the vocabulary size (the number of unique words).]]></explanation>
            <option_groups randomize="true">
              <option_group select="all">
                <option id="283966c37410fce58fc269d13b3381a9" selected_score="0" unselected_score="0">
                  <text><![CDATA[comedy]]></text>
                  <explanation><![CDATA[]]></explanation>
                </option>
                <option id="f9b1ee4d3dad4e6f5a18147512bdbc42" selected_score="1" unselected_score="0">
                  <text><![CDATA[action]]></text>
                  <explanation><![CDATA[]]></explanation>
                </option>
              </option_group>
            </option_groups>
          </data>
        </question>
        <question id="95941c15e28af765a71ef21697ef10ee" type="GS_Choice_Answer_Question">
          <metadata>
            <parameters>
              <rescale_score>1</rescale_score>
              <choice_type>radio</choice_type>
            </parameters>
          </metadata>
          <data>
            <text><![CDATA[<p>Suppose we have the following short movie reviews, each labeled with a genre, either <em>comedy</em> or <em>action</em>.</p>
<table>
<tr><td>1.</td><td>fun, couple, love, love</td><td><b>comedy</b></td></tr>
<tr><td>2.</td><td>fast, furious, shoot</td><td><b>action</b></td></tr>
<tr><td>3.</td><td>couple, fly, fast, fun, fun</td><td><b>comedy</b></td></tr>
<tr><td>4.</td><td>furious, shoot, shoot, fun</td><td><b>action</b></td></tr>
<tr><td>5.</td><td>fly, fast, shoot, love</td><td><b>action</b></td></tr>
</table>
<p>Now consider the following new review...</p>
<pre>
fast, furious, fun
</pre>
Using a simple $$Naïve$$ $$Bayes$$ approach with $$Laplace$$ $$Smoothing$$, how would we classify this, <em>comedy</em> or <em>action</em>? <b><em>(Hint: Don't forget to include the prior.)]]></text>
            <explanation><![CDATA[(See lecture slide #44 in the slide set on Text Classification and Naïve Bayes.) If <em>r</em> is our new review, we're looking for the class (genre) that maximizes (argmax) <em>P(c|r)</em>, where <em>P(c|r)</em> is proportional to the prior probability of the class, <em>P(c)</em>, times the product of <em>P(w|c)</em> for each word in <em>r</em>. <em>P(c)</em> is the number of reviews for each genre, divided by the total number of reviews. <em>P(w|c)</em> is the count of word <em>w</em> in class <em>c</em> plus 1 (i.e., smoothed), divided by the total count for <em>w</em> plus the vocabulary size (the number of unique words).]]></explanation>
            <option_groups randomize="true">
              <option_group select="all">
                <option id="d881e47f9bfa824a2bf10804e8697b96" selected_score="0" unselected_score="0">
                  <text><![CDATA[comedy]]></text>
                  <explanation><![CDATA[]]></explanation>
                </option>
                <option id="e7f93d5942db3582289e2d18d3980e43" selected_score="1" unselected_score="0">
                  <text><![CDATA[action]]></text>
                  <explanation><![CDATA[]]></explanation>
                </option>
              </option_group>
            </option_groups>
          </data>
        </question>
      </question_group>
      <question_group select="1">
        <preamble><![CDATA[]]></preamble>
        <question id="aeef142288f28222b2e5f5b2054b0ab0" type="GS_Short_Answer_Question_Simple">
          <metadata>
            <parameters>
              <rescale_score>1</rescale_score>
              <type>numeric</type>
            </parameters>
          </metadata>
          <data>
            <text><![CDATA[<p>As part of learning a sentiment lexicon for analyzing movie reviews, you decide to use the $$Turney$$ $$Polarity$$ method to compute the (real-valued) polarity of the phrase $$''special$$ $$effects''$$ using $$Pointwise$$ $$Mutual$$ $$Information$$ relative to the sentiment words $$''good''$$ and $$''bad''$$.

<p>Here are some actual counts from a corpus of IMDB reviews, where <b>NEAR</b> means "within 10 words of".</p>
<table>
<tr><td> </td><td><b><u>count</u></b></td></tr>
<tr><td>corpus size (<em>N</em>)</td><td>1,595,494</td></tr>
<tr><td>unique types (<em>V</em>)</td><td>46,060</td></tr>
<tr><td>"special effects"</td><td>437</td></tr>
<tr><td>"good"</td><td>3124</td></tr>
<tr><td>"bad"</td><td>1791</td></tr>
<tr><td>"special effects" <b>NEAR</b> "good"</td><td>36</td></tr>
<tr><td>"special effects" <b>NEAR</b> "bad"</td><td>18</td></tr>
</table>

<p>Your task is to calculate the value of $$Turney's$$ $$Polarity(''special$$ $$effects'')$$ with regard to $$''good''$$ and $$''bad''$$.<br>
<b>(signed numerical response rounded to the nearest tenth, e.g., 0.1, –0.7, etc.)</b></p>]]></text>
            <explanation><![CDATA[See lecture slide #60 in the section "Learning Sentiment Lexicons". For this calculation, total size of corpus, vocabulary size, and standalone count of "special effects" are not needed. <em>Polarity("special effects") = log<sub>2</sub>[ (count("special effects" NEAR "good") * count("bad")) / (count("special effects" NEAR "bad") * count("good")) ]</em>.]]></explanation>
            <option_groups randomize="true">
              <option_group select="all">
                <option id="46510fd78badaf7247ec0ff025450e1d" selected_score="1" unselected_score="0">
                  <text><![CDATA[0.2]]></text>
                  <explanation><![CDATA[Option explanation]]></explanation>
                </option>
              </option_group>
            </option_groups>
          </data>
        </question>
      </question_group>
      <question_group select="1">
        <preamble><![CDATA[]]></preamble>
        <question id="ef4180af3a58737fbc70bf6974bbe491" type="GS_Choice_Answer_Question">
          <metadata>
            <parameters>
              <rescale_score>1</rescale_score>
              <choice_type>radio</choice_type>
            </parameters>
          </metadata>
          <data>
            <text><![CDATA[<p>Once again we'll look at sentiment in movie reviews, but this time we'll see if our analysis yields a different result with standard $$Naïve$$ $$Bayes$$  vs. a $$Binarized$$ $$(Boolean$$ $$feature)$$ $$Naïve$$ $$Bayes$$ approach.</p>

<p>We train our classifier with documents having the following <b><em>counts</em></b> for key sentiment words, with <em>positive</em> or <em>negative</em> class assigned as noted.</p>

<table>
<tr><td> </td><td><em>"good"</em></td><td><em>"poor"</em></td><td><em>"great"</em></td><td>(class)</td></tr>
<tr><td>d1.</td><td>3</td><td>0</td><td>3</td><td><em>pos</em></td></tr>
<tr><td>d2.</td><td>0</td><td>1</td><td>2</td><td><em>pos</em></td></tr>
<tr><td>d3.</td><td>1</td><td>3</td><td>0</td><td><em>neg</em></td></tr>
<tr><td>d4.</td><td>1</td><td>5</td><td>2</td><td><em>neg</em></td></tr>
<tr><td>d5.</td><td>0</td><td>2</td><td>0</td><td><em>neg</em></td></tr>
</table>

<p>Now consider the following new review.</p>
<pre>Good acting, poor plot.</pre>

<p>Your task is to assign <em>positive</em> or <em>negative</em> sentiment first using standard $$Naïve$$ $$Bayes$$  then with a  $$Binarized$$ $$Naïve$$ $$Bayes$$ approach. Do the different approaches yield the same result in this case? Which of the following matches the pattern of your results? <em>(Hint: Use Add-1 smoothing with <b>both</b> methods.)</p>

<p><b>(response: [<em>standard</em>] / [<em>binarized</em>] )</b></p>]]></text>
            <explanation><![CDATA[(See lecture slide on "Binarized (Boolean feature)  Multinomial Naïve Bayes".) Use the same method in each case, except that for the $$Binarized$$ approach, only count each word a maximum of once per document, i.e., transform the table of counts above to 0 or 1 for each cell.]]></explanation>
            <option_groups randomize="true">
              <option_group select="all">
                <option id="cff8bab45a28882b12717fd298eaa05a" selected_score="0" unselected_score="0">
                  <text><![CDATA[pos / pos]]></text>
                  <explanation><![CDATA[Option explanation]]></explanation>
                </option>
                <option id="767ab9f5466a14c40743614f40d14de6" selected_score="0" unselected_score="0">
                  <text><![CDATA[pos / neg]]></text>
                  <explanation><![CDATA[Option explanation]]></explanation>
                </option>
                <option id="116fc7d5e435153366e0071820d5d6cf" selected_score="0" unselected_score="0">
                  <text><![CDATA[neg / pos]]></text>
                  <explanation><![CDATA[Option explanation]]></explanation>
                </option>
                <option id="d58b2b9ed93757a6d450a9e0813513e9" selected_score="1" unselected_score="0">
                  <text><![CDATA[neg / neg]]></text>
                  <explanation><![CDATA[Option explanation]]></explanation>
                </option>
              </option_group>
            </option_groups>
          </data>
        </question>
        <question id="fddb7a47a4fd93f261fa0a690b37716f" type="GS_Choice_Answer_Question">
          <metadata>
            <parameters>
              <rescale_score>1</rescale_score>
              <choice_type>radio</choice_type>
            </parameters>
          </metadata>
          <data>
            <text><![CDATA[<p>Once again we'll look at sentiment in movie reviews, but this time we'll see if our analysis yields a different result with standard $$Naïve$$ $$Bayes$$  vs. a $$Binarized$$ $$(Boolean$$ $$feature)$$ $$Naïve$$ $$Bayes$$ approach.</p>

<p>We train our classifier with documents having the following <b><em>counts</em></b> for key sentiment words, with <em>positive</em> or <em>negative</em> class assigned as noted.</p>

<table>
<tr><td> </td><td><em>"good"</em></td><td><em>"poor"</em></td><td><em>"great"</em></td><td>(class)</td></tr>
<tr><td>d1.</td><td>3</td><td>0</td><td>3</td><td><em>pos</em></td></tr>
<tr><td>d2.</td><td>0</td><td>1</td><td>2</td><td><em>pos</em></td></tr>
<tr><td>d3.</td><td>1</td><td>3</td><td>0</td><td><em>neg</em></td></tr>
<tr><td>d4.</td><td>1</td><td>5</td><td>2</td><td><em>neg</em></td></tr>
<tr><td>d5.</td><td>0</td><td>2</td><td>0</td><td><em>neg</em></td></tr>
</table>

<p>Now consider the following new review.</p>
<pre>A good, good plot and great characters, but poor acting.</pre>

<p>Your task is to assign <em>positive</em> or <em>negative</em> sentiment first using standard $$Naïve$$ $$Bayes$$  then with a  $$Binarized$$ $$Naïve$$ $$Bayes$$ approach. Do the different approaches yield the same result in this case? Which of the following matches the pattern of your results? <em>(Hint: Use Add-1 smoothing with <b>both</b> methods.)</p>

<p><b>(response: [<em>standard</em>] / [<em>binarized</em>] )</b></p>]]></text>
            <explanation><![CDATA[(See lecture slide on "Binarized (Boolean feature)  Multinomial Naïve Bayes".) Use the same method in each case, except that for the $$Binarized$$ approach, only count each word a maximum of once per document, i.e., transform the table of counts above to 0 or 1 for each cell.]]></explanation>
            <option_groups randomize="true">
              <option_group select="all">
                <option id="eccb4e2121cc635283a87b918a15fabe" selected_score="1" unselected_score="0">
                  <text><![CDATA[pos / pos]]></text>
                  <explanation><![CDATA[Option explanation]]></explanation>
                </option>
                <option id="7c8283027e775a9227735066f0dd0b72" selected_score="1" unselected_score="0">
                  <text><![CDATA[pos / neg]]></text>
                  <explanation><![CDATA[Option explanation]]></explanation>
                </option>
                <option id="d64cdca2de02ebb7ca7ce07d43ce2c9f" selected_score="0" unselected_score="0">
                  <text><![CDATA[neg / pos]]></text>
                  <explanation><![CDATA[Option explanation]]></explanation>
                </option>
                <option id="b40a9797a334eb5e51fa3aa774780eeb" selected_score="0" unselected_score="0">
                  <text><![CDATA[neg / neg]]></text>
                  <explanation><![CDATA[Option explanation]]></explanation>
                </option>
              </option_group>
            </option_groups>
          </data>
        </question>
      </question_group>
    </question_groups>
  </data>
</quiz>

