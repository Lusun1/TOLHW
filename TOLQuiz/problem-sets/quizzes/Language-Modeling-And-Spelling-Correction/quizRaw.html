

<!DOCTYPE html>
<html lang="en-US">

<head>
<meta charset="utf-8" />
<title></title>

<link rel="stylesheet" href="https://class.coursera.org/assets/core/css/bootstrap.min.css" />
<link rel="stylesheet" href="https://class.coursera.org/assets/core/css/main.css" />
<link rel="stylesheet" href="https://spark-public.s3.amazonaws.com/nlp/static/css/main.css" />
<link rel="stylesheet" href="https://class.coursera.org/assets/core/css/font-awesome.css" />
<script type="text/javascript" src="https://class.coursera.org/assets/core/js/jquery.js"></script>
<script type="text/javascript" src="https://class.coursera.org/assets/core/js/json.js"></script>
<script type="text/javascript" src="https://class.coursera.org/assets/core/js/hc_fix.js"></script>
<script type="text/javascript" src="https://class.coursera.org/assets/app/admin/i18n/js/i18n_editor.js"></script>
<script src="https://class.coursera.org/assets/core/js/bootstrap/bootstrap-modal.js" type="text/javascript"></script>
<script src="https://class.coursera.org/assets/core/js/modal_focus.js" type="text/javascript"></script>
<script src="https://class.coursera.org/assets/core/js/bind_modal_focus.js" type="text/javascript"></script>
<link rel="stylesheet" href="https://class.coursera.org/assets/app/admin/tooltips/css/tooltips.css">
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-28377374-1']);
  _gaq.push(['_setDomainName', 'coursera.org']);
  _gaq.push(['_setAllowLinker', true]);  
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script><link rel="icon" href="https://spark-public.s3.amazonaws.com/nlp/static/images/favicon.ico" />

</head>

<body>

    <a href="#page-content" class="hidden">Skip Navigation</a>

	
<div class="topbar">
    <div class="topbar-inner">
        <div class="container-fluid"
                          style="background-color: #822"
                          >
            <a target="_new" href="http://www.coursera.org/" class="brand">
				Coursera				<span style="background-color:#387690;padding-top:100px" class="label warning">Beta</span>
			</a>
            <ul class="nav secondary-nav">
                                                    <li style="padding: 10px 10px 11px; color:#BFBFBF; font-size:14px; border-right: solid 1px #333">Jason Bau</li>
                    <li><a href="https://class.coursera.org/nlp-staging/class/preferences">Preferences</a></li>
                    <li><a href="http://www.coursera.org/">All Courses</a></li>

                                            <li><a href="https://class.coursera.org/nlp-staging/admin/index">Admin</a></li>
                        <li><a href="https://class.coursera.org/nlp-staging/admin/support_external">Support</a></li>
                                        
                                            <li><a id="i18n_editor" href="#">I18N Editor</a></li>
                    
                    <li><a href="https://class.coursera.org/nlp-staging/class/aboutus">About</a></li>
                    <li><a href="https://class.coursera.org/nlp-staging/auth/feedback">Contact Us</a></li>
                    <li><a href="https://class.coursera.org/nlp-staging/auth/logout">Logout</a></li>
                            </ul>
        </div>
    </div>
</div>


    <div id="banner-top">
    	<div id="course-logo-text">
        	<a href="https://class.coursera.org/nlp-staging/class/index">
        		<img border="0" alt="Natural Language Processing" src="https://spark-public.s3.amazonaws.com/nlp/static/images/course-logo-text.png" />
        	</a>
        </div>
    	<div id="banner-course-info">
        	<div class="course-instructor-name">Dan Jurafsky, <span style="font-weight:normal">Professor of Linguistics</span><br />
Chris Manning, <span style="font-weight:normal">Associate Professor of Computer Science</span></div>
            <div class="course-time"></div>
        </div>
    </div>
    
        <div class="container-fluid">
        <div class="sidebar page-sidebar" id="page-sidebar">
            <h3><a href="https://class.coursera.org/nlp-staging/admin/index">Administration</a></h3>

<b>General</b>
<ul>
	<li><a href="https://class.coursera.org/nlp-staging/admin/courseadmin/index">Course Settings</a></li>
		<li><a href="https://class.coursera.org/nlp-staging/admin/staging/index">Deployment &amp; Staging </a></li>
		<li><a href="https://class.coursera.org/nlp-staging/admin/grading_policy/index">Grading Policy</a></li> 
		<li><a href="https://class.coursera.org/nlp-staging/admin/navbar/index">Navigation Bar Settings</a></li>
	            <li><a href="https://class.coursera.org/nlp-staging/admin/announcement/index">Announcement Management</a></li>
        </ul>

<b>Content Management</b>
<ul>
	<li><a href="https://class.coursera.org/nlp-staging/admin/quiz/index">Quiz Management</a></li>
  <li><a href="https://class.coursera.org/nlp-staging/admin/assignment/index">Assignment Management</a></li>
<!--
  <li><a href="https://class.coursera.org/nlp-staging/admin/creative_assignments/index">Creative Assignments</a></li>
-->
	<li><a href="https://class.coursera.org/nlp-staging/admin/lecture/index">Lecture Management</a></li>
	<li><a href="https://class.coursera.org/nlp-staging/admin/section/index">Section Management</a></li>
	<li><a href="https://class.coursera.org/nlp-staging/admin/courseadmin/announcement">Upcoming Items Display</a></li>
</ul>
<b>Platform Services</b>
<ul>
        <li><a href="https://class.coursera.org/nlp-staging/admin/assets/index">Asset Administration</a></li>
  	<li><a href="https://class.coursera.org/nlp-staging/admin/forum/index">Forum Administration</a></li>
    	<li><a href="https://class.coursera.org/nlp-staging/admin/email/index">Email Users</a></li>	
</ul>

<b>Advanced Tools</b>
<ul>
	<li><a href="https://class.coursera.org/nlp-staging/admin/queue/index">Queue Administration</a></li>
   
    	<li><a href="https://class.coursera.org/nlp-staging/admin/staging/time_shift">Chronos</a></li>
    	    <li><a href="https://class.coursera.org/nlp-staging/admin/log/index">Log Viewer</a></li>
    		<li><a href="https://class.coursera.org/nlp-staging/admin/user/index">User Administration</a></li>
	</ul>

<b>Import Tools</b>
<ul>
	<li><a href="https://class.coursera.org/nlp-staging/admin/assignment/import">Assignment Submissions</a></li>
</ul>
<b>Export Tools</b>
<ul>
	<li><a href="https://class.coursera.org/nlp-staging/admin/export/quiz_responses">Quiz Summary</a></li>
	<li><a href="https://class.coursera.org/nlp-staging/admin/export/detailed_quiz_responses">Detailed Quiz Responses</a></li>
	<li><a href="https://class.coursera.org/nlp-staging/admin/export/assignment_submissions">Assignment Submissions</a></li>
	<li><a href="https://class.coursera.org/nlp-staging/admin/export/gradebook">Class Gradebook</a></li>
</ul>

<b>Status Monitoring</b>
<ul>
    <li><a href="https://class.coursera.org/nlp-staging/admin/stats/activity">Activity Tracking</a></li>
    <li><a href="https://class.coursera.org/nlp-staging/admin/stats/export">Export Statistics</a></li> 
        <li><a href="https://class.coursera.org/nlp-staging/admin/lecture/recode_status">Video Status</a></li>
</ul>
        </div>
        <div class="content page-content" id="page-content">
            <script src="https://class.coursera.org/assets/app/admin/quiz/codemirror/lib/codemirror.js" type="text/javascript"></script>
<script src="https://class.coursera.org/assets/app/admin/quiz/codemirror/mode/xml/xml.js" type="text/javascript"></script>
<link rel="stylesheet" type="text/css" href="https://class.coursera.org/assets/app/admin/quiz/codemirror/lib/codemirror.css"/>

<form method="post" action="https://class.coursera.org/nlp-staging/admin/quiz/raw_save">
<textarea rows="24" cols="96" style="width: 640px" id="quiz_xml" name="quiz_xml">
&lt;quiz&gt;
  &lt;metadata&gt;
    &lt;title&gt;Language Modeling and Spelling Correction&lt;/title&gt;
    &lt;open_time&gt;2012-03-17 0001&lt;/open_time&gt;
    &lt;soft_close_time&gt;2012-04-03 2359&lt;/soft_close_time&gt;
    &lt;hard_close_time&gt;2012-05-22 2359&lt;/hard_close_time&gt;
    &lt;duration&gt;0&lt;/duration&gt;
    &lt;retry_delay&gt;10&lt;/retry_delay&gt;
    &lt;maximum_submissions&gt;5&lt;/maximum_submissions&gt;
    &lt;modified_time&gt;1335394022319&lt;/modified_time&gt;
    &lt;parameters&gt;
      &lt;show_explanations&gt;
        &lt;question&gt;before_hard_close_time&lt;/question&gt;
        &lt;option&gt;before_soft_close_time&lt;/option&gt;
        &lt;score&gt;before_soft_close_time&lt;/score&gt;
      &lt;/show_explanations&gt;
    &lt;/parameters&gt;
    &lt;maximum_score&gt;5&lt;/maximum_score&gt;
  &lt;/metadata&gt;
  &lt;preamble&gt;&lt;![CDATA[]]&gt;&lt;/preamble&gt;
  &lt;data&gt;
    &lt;question_groups&gt;
      &lt;question_group select=&quot;1&quot;&gt;
        &lt;preamble&gt;&lt;![CDATA[]]&gt;&lt;/preamble&gt;
        &lt;question id=&quot;7b52178db4088bc6f74fe545b14a2815&quot; type=&quot;GS_Choice_Answer_Question&quot;&gt;
          &lt;metadata&gt;
            &lt;parameters&gt;
              &lt;rescale_score&gt;1&lt;/rescale_score&gt;
              &lt;choice_type&gt;radio&lt;/choice_type&gt;
            &lt;/parameters&gt;
          &lt;/metadata&gt;
          &lt;data&gt;
            &lt;text&gt;&lt;![CDATA[We are given the following corpus, similar to the one in lecture but with &quot;ham&quot; replaced by &quot;Sam&quot; and &quot;I am Sam&quot; included twice:&lt;br&gt;

            &lt;ul&gt;
            &lt;li&gt;&amp;lt;s&amp;gt; I am Sam &amp;lt;/s&amp;gt;&lt;br&gt;&lt;/li&gt;
            &lt;li&gt;&amp;lt;s&amp;gt; Sam I am &amp;lt;/s&amp;gt;&lt;br&gt;&lt;/li&gt;
            &lt;li&gt;&amp;lt;s&amp;gt; I am Sam &amp;lt;/s&amp;gt;&lt;br&gt;&lt;/li&gt;
            &lt;li&gt;&amp;lt;s&amp;gt; I do not like green eggs and Sam &amp;lt;/s&amp;gt;&lt;/li&gt;
&lt;/ul&gt;

Using a bigram language model with add-one smoothing, what is P(Sam | am)? Include &amp;lt;s&amp;gt; and  &amp;lt;/s&amp;gt; in your counts just like any other token.]]&gt;&lt;/text&gt;
            &lt;explanation&gt;&lt;![CDATA[Using a bigram language model with add-one smoothing, $$P(Sam | am) = \frac{c(am, Sam) + 1}{c(am) + V} = \frac{2 + 1}{3 + 11} = \frac{3}{14}$$.]]&gt;&lt;/explanation&gt;
            &lt;option_groups randomize=&quot;true&quot;&gt;
              &lt;option_group select=&quot;all&quot;&gt;
                &lt;option id=&quot;0065c0c6cfd40373e296e5d00f2067a8&quot; selected_score=&quot;1&quot; unselected_score=&quot;0&quot;&gt;
                  &lt;text&gt;&lt;![CDATA[$$\frac{3}{14}$$]]&gt;&lt;/text&gt;
                  &lt;explanation&gt;&lt;![CDATA[This is the correct answer.]]&gt;&lt;/explanation&gt;
                &lt;/option&gt;
                &lt;option id=&quot;f02626ed6c601ed824d99d241020c61b&quot; selected_score=&quot;0&quot; unselected_score=&quot;0&quot;&gt;
                  &lt;text&gt;&lt;![CDATA[$$\frac{2}{3}$$]]&gt;&lt;/text&gt;
                  &lt;explanation&gt;&lt;![CDATA[This is the maximum-likelihood estimate.]]&gt;&lt;/explanation&gt;
                &lt;/option&gt;
                &lt;option id=&quot;7f257d8cf6f371d9ac51aad29381b5a3&quot; selected_score=&quot;0&quot; unselected_score=&quot;0&quot;&gt;
                  &lt;text&gt;&lt;![CDATA[$$\frac{3}{28}$$]]&gt;&lt;/text&gt;
                  &lt;explanation&gt;&lt;![CDATA[Here $$V$$ denotes the size of the vocabulary, which is the number if types in the corpus, not the number of tokens.]]&gt;&lt;/explanation&gt;
                &lt;/option&gt;
                &lt;option id=&quot;c859d48035c909308e5ca08edacd96b2&quot; selected_score=&quot;0&quot; unselected_score=&quot;0&quot;&gt;
                  &lt;text&gt;&lt;![CDATA[$$\frac{2}{14}$$]]&gt;&lt;/text&gt;
                  &lt;explanation&gt;&lt;![CDATA[We add one to all counts, even those which are already non-zero.]]&gt;&lt;/explanation&gt;
                &lt;/option&gt;
              &lt;/option_group&gt;
            &lt;/option_groups&gt;
          &lt;/data&gt;
        &lt;/question&gt;
        &lt;question id=&quot;fb1b7fa21405b70e07ccac751ef7941d&quot; type=&quot;GS_Choice_Answer_Question&quot;&gt;
          &lt;metadata&gt;
            &lt;parameters&gt;
              &lt;rescale_score&gt;1&lt;/rescale_score&gt;
              &lt;choice_type&gt;radio&lt;/choice_type&gt;
            &lt;/parameters&gt;
          &lt;/metadata&gt;
          &lt;data&gt;
            &lt;text&gt;&lt;![CDATA[We are given the following corpus, similar to the one in lecture but with &quot;ham&quot; replaced by &quot;Sam&quot;  and &quot;I am Sam&quot; included twice:&lt;br&gt;

            &lt;ul&gt;
            &lt;li&gt;&amp;lt;s&amp;gt; I am Sam &amp;lt;/s&amp;gt;&lt;br&gt;&lt;/li&gt;
            &lt;li&gt;&amp;lt;s&amp;gt; Sam I am &amp;lt;/s&amp;gt;&lt;br&gt;&lt;/li&gt;
            &lt;li&gt;&amp;lt;s&amp;gt; I am Sam &amp;lt;/s&amp;gt;&lt;br&gt;&lt;/li&gt;
            &lt;li&gt;&amp;lt;s&amp;gt; I do not like green eggs and Sam &amp;lt;/s&amp;gt;&lt;/li&gt;
&lt;/ul&gt;

Using a bigram language model with add-one smoothing, what is P(Sam | eggs)? Include &amp;lt;s&amp;gt; and  &amp;lt;/s&amp;gt; in your counts just like any other token.]]&gt;&lt;/text&gt;
            &lt;explanation&gt;&lt;![CDATA[Using a bigram language model with add-one smoothing, $$P(Sam | eggs) = \frac{c(eggs, Sam) + 1}{c(eggs) + V} = \frac{0 + 1}{1 + 11} = \frac{1}{12}$$.]]&gt;&lt;/explanation&gt;
            &lt;option_groups randomize=&quot;true&quot;&gt;
              &lt;option_group select=&quot;all&quot;&gt;
                &lt;option id=&quot;e6c3b0ad86e27b2d2e82caa76d25aa04&quot; selected_score=&quot;1&quot; unselected_score=&quot;0&quot;&gt;
                  &lt;text&gt;&lt;![CDATA[$$\frac{1}{12}$$]]&gt;&lt;/text&gt;
                  &lt;explanation&gt;&lt;![CDATA[This is the correct answer.]]&gt;&lt;/explanation&gt;
                &lt;/option&gt;
                &lt;option id=&quot;ce345b7aeb422723d62b0ed893c03d92&quot; selected_score=&quot;0&quot; unselected_score=&quot;0&quot;&gt;
                  &lt;text&gt;&lt;![CDATA[$$0$$]]&gt;&lt;/text&gt;
                  &lt;explanation&gt;&lt;![CDATA[This is the maximum-likelihood estimate.]]&gt;&lt;/explanation&gt;
                &lt;/option&gt;
                &lt;option id=&quot;37b97e447d51104337cb5140df04044b&quot; selected_score=&quot;0&quot; unselected_score=&quot;0&quot;&gt;
                  &lt;text&gt;&lt;![CDATA[$$1$$]]&gt;&lt;/text&gt;
                  &lt;explanation&gt;&lt;![CDATA[When we add $$1$$ to each count we must add $$V$$ to the denominator.]]&gt;&lt;/explanation&gt;
                &lt;/option&gt;
                &lt;option id=&quot;7099bfb0d11898298ebd69f705e24769&quot; selected_score=&quot;0&quot; unselected_score=&quot;0&quot;&gt;
                  &lt;text&gt;&lt;![CDATA[$$\frac{5}{12}$$]]&gt;&lt;/text&gt;
                  &lt;explanation&gt;&lt;![CDATA[In the numerator we use $$c(eggs, Sam)$$, not $$c(Sam)$$.]]&gt;&lt;/explanation&gt;
                &lt;/option&gt;
              &lt;/option_group&gt;
            &lt;/option_groups&gt;
          &lt;/data&gt;
        &lt;/question&gt;
        &lt;question id=&quot;a6a88070036dc6018df493ce845e6576&quot; type=&quot;GS_Choice_Answer_Question&quot;&gt;
          &lt;metadata&gt;
            &lt;parameters&gt;
              &lt;rescale_score&gt;1&lt;/rescale_score&gt;
              &lt;choice_type&gt;radio&lt;/choice_type&gt;
            &lt;/parameters&gt;
          &lt;/metadata&gt;
          &lt;data&gt;
            &lt;text&gt;&lt;![CDATA[We are given the following corpus, similar to the one in lecture but with &quot;ham&quot; replaced by &quot;Sam&quot;  and &quot;I am Sam&quot; included twice:&lt;br&gt;

            &lt;ul&gt;
            &lt;li&gt;&amp;lt;s&amp;gt; I am Sam &amp;lt;/s&amp;gt;&lt;br&gt;&lt;/li&gt;
            &lt;li&gt;&amp;lt;s&amp;gt; Sam I am &amp;lt;/s&amp;gt;&lt;br&gt;&lt;/li&gt;
            &lt;li&gt;&amp;lt;s&amp;gt; I am Sam &amp;lt;/s&amp;gt;&lt;br&gt;&lt;/li&gt;
            &lt;li&gt;&amp;lt;s&amp;gt; I do not like green eggs and Sam &amp;lt;/s&amp;gt;&lt;/li&gt;
&lt;/ul&gt;

Using a bigram language model with add-one smoothing, what is P(am | I)? Include &amp;lt;s&amp;gt; and  &amp;lt;/s&amp;gt; in your counts just like any other token.]]&gt;&lt;/text&gt;
            &lt;explanation&gt;&lt;![CDATA[Using a bigram language model with add-one smoothing, $$P(am | I) = \frac{c(I, am) + 1}{c(I) + V} = \frac{3 + 1}{4 + 11} = \frac{4}{15}$$.]]&gt;&lt;/explanation&gt;
            &lt;option_groups randomize=&quot;true&quot;&gt;
              &lt;option_group select=&quot;all&quot;&gt;
                &lt;option id=&quot;87f8a221947408d6ac228e80620885c4&quot; selected_score=&quot;1&quot; unselected_score=&quot;0&quot;&gt;
                  &lt;text&gt;&lt;![CDATA[$$\frac{4}{15}$$]]&gt;&lt;/text&gt;
                  &lt;explanation&gt;&lt;![CDATA[This is the correct answer.]]&gt;&lt;/explanation&gt;
                &lt;/option&gt;
                &lt;option id=&quot;4b94c4342d8fde3b08db8970d1a9ba95&quot; selected_score=&quot;0&quot; unselected_score=&quot;0&quot;&gt;
                  &lt;text&gt;&lt;![CDATA[$$\frac{3}{4}$$]]&gt;&lt;/text&gt;
                  &lt;explanation&gt;&lt;![CDATA[This is the maximum-likelihood estimate.]]&gt;&lt;/explanation&gt;
                &lt;/option&gt;
                &lt;option id=&quot;8da941ca36a0a298d4d8dddba766619c&quot; selected_score=&quot;0&quot; unselected_score=&quot;0&quot;&gt;
                  &lt;text&gt;&lt;![CDATA[$$1$$]]&gt;&lt;/text&gt;
                  &lt;explanation&gt;&lt;![CDATA[When we add one to each count we must add $$V$$ to the denominator.]]&gt;&lt;/explanation&gt;
                &lt;/option&gt;
                &lt;option id=&quot;0f39f79b50fffc484f783cedc9658eb2&quot; selected_score=&quot;0&quot; unselected_score=&quot;0&quot;&gt;
                  &lt;text&gt;&lt;![CDATA[$$\frac{4}{24}$$]]&gt;&lt;/text&gt;
                  &lt;explanation&gt;&lt;![CDATA[The term $$V$$ measures the number of word types in the corpus, not the number of tokens.]]&gt;&lt;/explanation&gt;
                &lt;/option&gt;
              &lt;/option_group&gt;
            &lt;/option_groups&gt;
          &lt;/data&gt;
        &lt;/question&gt;
      &lt;/question_group&gt;
      &lt;question_group select=&quot;1&quot;&gt;
        &lt;preamble&gt;&lt;![CDATA[]]&gt;&lt;/preamble&gt;
        &lt;question id=&quot;be557108dfe0083df187b0b3e45b9b76&quot; type=&quot;GS_Choice_Answer_Question&quot;&gt;
          &lt;metadata&gt;
            &lt;parameters&gt;
              &lt;rescale_score&gt;1&lt;/rescale_score&gt;
              &lt;choice_type&gt;radio&lt;/choice_type&gt;
            &lt;/parameters&gt;
          &lt;/metadata&gt;
          &lt;data&gt;
            &lt;text&gt;&lt;![CDATA[Suppose we want to smooth the likelihood term of a noisy channel model of spelling. We are given two words, $$x$$ and $$w$$, where $$x$$ is the same as $$w$$, except the letter $$w_{i-1}$$ in $$w$$ has been miss typed as $$w_{i-1}x_i$$ in $$x$$.

Specifically, we want to apply add-one smoothing to $$P(x|w)$$, the probability of typing $$w_{i-1}x_i$$ instead of $$w_{i-1}$$, where $$x_i$$ and $$w_{i-1}$$ are single letters.

For insertions, $$P(x|w) = \frac{ins[w_{i-1}, x_i]}{c(w_{i-1})}$$, where $$ins[w_{i-1}, x_i]$$ is the number of times that $$x_i$$ is inserted after $$w_{i-1}$$ in the corpus, and $$c(w_{i-1})$$ is the number of times letter $$w_{i-1}$$ appears in our corpus. Again, please note that here $$x_i$$ and $$w_{i-1}$$ are individual letters, not words. 
&lt;p&gt;
What is the formula for $$P(x|w)$$ if we use add-one smoothing to the insertion edit model? Assume the only characters we use are lowercase a-z, that there are $$V$$ word types in our corpus, and $$n$$ total characters, not counting spaces.]]&gt;&lt;/text&gt;
            &lt;explanation&gt;&lt;![CDATA[The distribution $$P(x|w)$$ has $$26$$ entries, one for each possible value of $$x_i$$. Thus, we add $$26$$ total fictional counts to our data, which means we must add $$26$$ to the denominator.]]&gt;&lt;/explanation&gt;
            &lt;option_groups randomize=&quot;true&quot;&gt;
              &lt;option_group select=&quot;all&quot;&gt;
                &lt;option id=&quot;16399831d1f0bce028ad32170ac541f3&quot; selected_score=&quot;0&quot; unselected_score=&quot;0&quot;&gt;
                  &lt;text&gt;&lt;![CDATA[$$\frac{ins[w_{i-1}, x_i]}{c(w_{i-1})}$$]]&gt;&lt;/text&gt;
                  &lt;explanation&gt;&lt;![CDATA[This is the maximum-likelihood estimate.]]&gt;&lt;/explanation&gt;
                &lt;/option&gt;
                &lt;option id=&quot;dc657c6e300521b498bfb5af59ade022&quot; selected_score=&quot;0&quot; unselected_score=&quot;0&quot;&gt;
                  &lt;text&gt;&lt;![CDATA[$$\frac{ins[w_{i-1}, x_i] + 1}{c(w_{i-1}) + V}$$]]&gt;&lt;/text&gt;
                  &lt;explanation&gt;&lt;![CDATA[The number of times we add one is independent of the vocabulary size, so the denominator is not correct.]]&gt;&lt;/explanation&gt;
                &lt;/option&gt;
                &lt;option id=&quot;9c784ae24bec5c24b45fa52ce5967002&quot; selected_score=&quot;1&quot; unselected_score=&quot;0&quot;&gt;
                  &lt;text&gt;&lt;![CDATA[$$\frac{ins[w_{i-1}, x_i] + 1}{c(w_{i-1}) + 26}$$]]&gt;&lt;/text&gt;
                  &lt;explanation&gt;&lt;![CDATA[This is the correct answer.]]&gt;&lt;/explanation&gt;
                &lt;/option&gt;
                &lt;option id=&quot;b09ee0f9fdcee1644ff379acf940a9af&quot; selected_score=&quot;0&quot; unselected_score=&quot;0&quot;&gt;
                  &lt;text&gt;&lt;![CDATA[$$\frac{ins[w_{i-1}, x_i] + 1}{c(w_{i-1}) + n}$$]]&gt;&lt;/text&gt;
                  &lt;explanation&gt;&lt;![CDATA[We compute P(x|w) for each letter, not for each occurrence of a letter in the corpus. Thus, adding $$n$$ to the denominator isn't correct.]]&gt;&lt;/explanation&gt;
                &lt;/option&gt;
              &lt;/option_group&gt;
            &lt;/option_groups&gt;
          &lt;/data&gt;
        &lt;/question&gt;
        &lt;question id=&quot;211b6349a5d19476e07f0e231666f413&quot; type=&quot;GS_Choice_Answer_Question&quot;&gt;
          &lt;metadata&gt;
            &lt;parameters&gt;
              &lt;rescale_score&gt;1&lt;/rescale_score&gt;
              &lt;choice_type&gt;radio&lt;/choice_type&gt;
            &lt;/parameters&gt;
          &lt;/metadata&gt;
          &lt;data&gt;
            &lt;text&gt;&lt;![CDATA[Suppose we want to smooth the likelihood term of a noisy channel model of spelling. We are given two words, $$x$$ and $$w$$, where $$x$$ is the same as $$w$$, except the two-letter sequence $$w_{i-1}w_i$$ in $$w$$ has been miss typed as just $$w_{i-1}$$ in $$x$$.

We want to apply add-one smoothing to $$P(x | w)$$, which is the probability of typing $$w_{i-1}$$ instead of $$w_{i-1}w_i$$, where $$w_{i-1}$$ and $$w_i$$ are single letters.

For deletions, the maximum-likelihood estimate of $$P(x | w) = \frac{del[w_{i-1}, w_i]}{c(w_{i-1}w_i)}$$, where $$del[w_{i-1}, w_i]$$ is the number of times that letter $$w_i$$ is deleted after letter $$w_{i-1}$$ in the corpus, and $$c(w_{i-1}w_i)$$ is the number of times the two letter sequence $$w_{i-1}w_i$$ appears in our corpus. Please note again that here $$w_{i-1}$$ and $$w_i$$ are individual letters, not whole words. 
&lt;p&gt;
What is the formula for $$P(x | w)$$ if we use add-one smoothing on the deletion edit model? Assume the only characters we use are lowercase a-z, that there are $$V$$ word types in our corpus, and $$n$$ total characters, not counting spaces.]]&gt;&lt;/text&gt;
            &lt;explanation&gt;&lt;![CDATA[The distribution $$P(x|w)$$ has $$26$$ entries, one for each possible value of $$w_i$$. Thus, we add $$26$$ total fictional counts to our data, which means we must add $$26$$ to the denominator.]]&gt;&lt;/explanation&gt;
            &lt;option_groups randomize=&quot;true&quot;&gt;
              &lt;option_group select=&quot;all&quot;&gt;
                &lt;option id=&quot;9691e9422dda97698c2b9b775e07c95c&quot; selected_score=&quot;0&quot; unselected_score=&quot;0&quot;&gt;
                  &lt;text&gt;&lt;![CDATA[$$\frac{del[w_{i-1}, w_i]}{c(w_{i-1}w_i)}$$]]&gt;&lt;/text&gt;
                  &lt;explanation&gt;&lt;![CDATA[This is the maximum-likelihood estimate.]]&gt;&lt;/explanation&gt;
                &lt;/option&gt;
                &lt;option id=&quot;e4c9b907bfd2cc30be69994ef3d5d420&quot; selected_score=&quot;0&quot; unselected_score=&quot;0&quot;&gt;
                  &lt;text&gt;&lt;![CDATA[$$\frac{del[w_{i-1}, w_i] + 1}{c(w_{i-1}w_i) + V}$$]]&gt;&lt;/text&gt;
                  &lt;explanation&gt;&lt;![CDATA[The number of times we add one is independent of the vocabulary size, so the denominator is not correct.]]&gt;&lt;/explanation&gt;
                &lt;/option&gt;
                &lt;option id=&quot;a7c6a1af1b67194a48aba1c813032c1a&quot; selected_score=&quot;1&quot; unselected_score=&quot;0&quot;&gt;
                  &lt;text&gt;&lt;![CDATA[$$\frac{del[w_{i-1}, w_i] + 1}{c(w_{i-1}w_i) + 26}$$]]&gt;&lt;/text&gt;
                  &lt;explanation&gt;&lt;![CDATA[This is the correct answer.]]&gt;&lt;/explanation&gt;
                &lt;/option&gt;
                &lt;option id=&quot;cfe77da859b2f45a315ac803e856723f&quot; selected_score=&quot;0&quot; unselected_score=&quot;0&quot;&gt;
                  &lt;text&gt;&lt;![CDATA[$$\frac{del[w_{i-1}, w_i] + 1}{c(w_{i-1}w_i) + n}$$]]&gt;&lt;/text&gt;
                  &lt;explanation&gt;&lt;![CDATA[We compute P(x|w) for each letter, not for each occurrence of a letter in the corpus. Thus, adding $$n$$ to the denominator isn't correct.]]&gt;&lt;/explanation&gt;
                &lt;/option&gt;
              &lt;/option_group&gt;
            &lt;/option_groups&gt;
          &lt;/data&gt;
        &lt;/question&gt;
        &lt;question id=&quot;5a6c612b24db893c1aa4c4817a749acc&quot; type=&quot;GS_Choice_Answer_Question&quot;&gt;
          &lt;metadata&gt;
            &lt;parameters&gt;
              &lt;rescale_score&gt;1&lt;/rescale_score&gt;
              &lt;choice_type&gt;radio&lt;/choice_type&gt;
            &lt;/parameters&gt;
          &lt;/metadata&gt;
          &lt;data&gt;
            &lt;text&gt;&lt;![CDATA[Suppose we want to smooth the likelihood term of a noisy channel model of spelling. We are given two words, $$x$$ and $$w$$, where $$x$$ is the same as $$w$$, except that the letter $$w_i$$ in $$w$$ has been miss typed as $$x_i$$ in $$x$$.

We want to apply add-one smoothing to $$P(x | w)$$, which is the probability of typing $$x_i$$ instead of $$w_i$$, where $$x_i$$ and $$w_i$$ are single letters.

For substitutions, the maximum-likelihood estimate of $$P(x | w) = \frac{sub[x_i, w_i]}{c(w_i)}$$, where $$sub[x_i, w_i]$$ is the number of times that $$x_i$$ is substituted for $$w_i$$ in the corpus, and $$c(w_i)$$ is the number of times that the letter $$w_i$$ appears in our corpus. Again, please note that here $$x_i$$ and $$w_i$$ are individual letters, not whole words. 
&lt;p&gt;
What is the formula for $$P(x | w)$$ if we use add-one smoothing on the substitution edit model? Assume the only characters we use are lowercase a-z, that there are $$V$$ word types in our corpus, and $$n$$ total characters, not counting spaces.]]&gt;&lt;/text&gt;
            &lt;explanation&gt;&lt;![CDATA[The distribution $$P(x|w)$$ has $$26$$ entries, one for each possible value of $$x_i$$. Thus, we add $$26$$ total fictional counts to our data, which means we must add $$26$$ to the denominator.]]&gt;&lt;/explanation&gt;
            &lt;option_groups randomize=&quot;true&quot;&gt;
              &lt;option_group select=&quot;all&quot;&gt;
                &lt;option id=&quot;a165ee42a3df7036dce722b69be97509&quot; selected_score=&quot;0&quot; unselected_score=&quot;0&quot;&gt;
                  &lt;text&gt;&lt;![CDATA[$$\frac{sub[x_i, w_i]}{c(w_i)}$$]]&gt;&lt;/text&gt;
                  &lt;explanation&gt;&lt;![CDATA[This is the maximum-likelihood estimate.]]&gt;&lt;/explanation&gt;
                &lt;/option&gt;
                &lt;option id=&quot;e6febc896728dcee72a4807776b4a2a1&quot; selected_score=&quot;0&quot; unselected_score=&quot;0&quot;&gt;
                  &lt;text&gt;&lt;![CDATA[$$\frac{sub[x_i, w_i] + 1}{c(w_i) + V}$$]]&gt;&lt;/text&gt;
                  &lt;explanation&gt;&lt;![CDATA[The number of times we add one is independent of the vocabulary size, so the denominator is not correct.]]&gt;&lt;/explanation&gt;
                &lt;/option&gt;
                &lt;option id=&quot;9b4f1322d1d5ed48a76ed54b5ec5d06d&quot; selected_score=&quot;1&quot; unselected_score=&quot;0&quot;&gt;
                  &lt;text&gt;&lt;![CDATA[$$\frac{sub[x_i, w_i] + 1}{c(w_i) + 26}$$]]&gt;&lt;/text&gt;
                  &lt;explanation&gt;&lt;![CDATA[This is the correct answer.]]&gt;&lt;/explanation&gt;
                &lt;/option&gt;
                &lt;option id=&quot;0e5b1dd96a6bf6b3fa47fc2458e8febd&quot; selected_score=&quot;0&quot; unselected_score=&quot;0&quot;&gt;
                  &lt;text&gt;&lt;![CDATA[$$\frac{sub[x_i, w_i] + 1}{c(w_i) + n}$$]]&gt;&lt;/text&gt;
                  &lt;explanation&gt;&lt;![CDATA[We compute P(x|w) for each letter in the alphabet, not for each occurrence of a letter in the corpus. Thus, adding $$n$$ to the denominator isn't correct.]]&gt;&lt;/explanation&gt;
                &lt;/option&gt;
              &lt;/option_group&gt;
            &lt;/option_groups&gt;
          &lt;/data&gt;
        &lt;/question&gt;
      &lt;/question_group&gt;
      &lt;question_group select=&quot;1&quot;&gt;
        &lt;preamble&gt;&lt;![CDATA[]]&gt;&lt;/preamble&gt;
        &lt;question id=&quot;0942711dac7795c40307d4689e1166cf&quot; type=&quot;GS_Choice_Answer_Question&quot;&gt;
          &lt;metadata&gt;
            &lt;parameters&gt;
              &lt;rescale_score&gt;1&lt;/rescale_score&gt;
              &lt;choice_type&gt;radio&lt;/choice_type&gt;
            &lt;/parameters&gt;
          &lt;/metadata&gt;
          &lt;data&gt;
            &lt;text&gt;&lt;![CDATA[We are given the following corpus, similar to the one in lecture but with &quot;ham&quot; replaced by &quot;Sam&quot;  and &quot;I am Sam&quot; included twice:&lt;br&gt;

            &lt;ul&gt;
            &lt;li&gt;&amp;lt;s&amp;gt; I am Sam &amp;lt;/s&amp;gt;&lt;br&gt;&lt;/li&gt;
            &lt;li&gt;&amp;lt;s&amp;gt; Sam I am &amp;lt;/s&amp;gt;&lt;br&gt;&lt;/li&gt;
            &lt;li&gt;&amp;lt;s&amp;gt; I am Sam &amp;lt;/s&amp;gt;&lt;br&gt;&lt;/li&gt;
            &lt;li&gt;&amp;lt;s&amp;gt; I do not like green eggs and Sam &amp;lt;/s&amp;gt;&lt;/li&gt;
&lt;/ul&gt;

Using interpolated Kneser-Ney smoothing, what is $$P_{KN}(Sam | am)$$ if we use a discount factor of $$d = 1$$? 
&lt;p&gt;
&lt;p&gt;Here are some quantities of interest to make this less tedious:

&lt;ul&gt;
&lt;li&gt; $$c(am, Sam) = 2$$ &lt;/li&gt;
&lt;li&gt; $$c(am) = 3$$ &lt;/li&gt;
&lt;li&gt;$$c(Sam) = 4$$ &lt;/li&gt;
&lt;!--&lt;li&gt; $$|\{w_{i-1} : c(w_{i-1}, am) &gt; 0\}| = 1$$ &lt;/li&gt; --&gt;
&lt;li&gt; $$|\{w : c(am, w) &gt; 0\}| = 2$$
&lt;li&gt; $$|\{(w_{j-1}, w_j) : c(w_{j-1}, w_j) &gt; 0\}| = 14$$
&lt;li&gt; $$|\{ w_{i-1} : c(w_{i-1}, \text{Sam}) &gt; 0 \}| = 3$$ &lt;/li&gt;
&lt;/ul&gt;

As a reminder, here is the formula for $$P_{KN}$$:

$$P_{KN}(w_i | w_{i-1}) = \frac{ \max(c(w_{i-1}, w_i) - d, 0)}{c(w_{i-1})} + \lambda(w_{i-1}) P_{CONTINUATION}(w_i)$$

where $$\lambda(w_{i-1}) = \frac{d}{c(w_{i-1})} |\{w : c(w_{i-1}, w) &gt; 0 \}|$$ 
and $$P_{CONTINUATION}(w_i) = \frac{|\{w_{i-1} : c(w_{i-1}, w_i) &gt; 0 \}|} {|\{(w_{j-1}, w_j) : c(w_{j-1}, w_j) &gt; 0 \}|}$$]]&gt;&lt;/text&gt;
            &lt;explanation&gt;&lt;![CDATA[$$P_{KN}(Sam | am) = \frac{ \max(c(am, Sam) - 1, 0)}{c(am)} +  \frac{1}{c(am)} |\{w : c(am, w) &gt; 0 \}|   \frac{|\{w_{i-1} : c(w_{i-1}, Sam) &gt; 0 \}|} {|\{(w_{j-1}, w_j) : c(w_{j-1}, w_j) &gt; 0 \}|} = \frac{2-1}{3} + \frac{2}{3}\cdot\frac{3}{14}$$]]&gt;&lt;/explanation&gt;
            &lt;option_groups randomize=&quot;true&quot;&gt;
              &lt;option_group select=&quot;all&quot;&gt;
                &lt;option id=&quot;7a26d7681dcbdac0ef63b249b7589b1c&quot; selected_score=&quot;1&quot; unselected_score=&quot;0&quot;&gt;
                  &lt;text&gt;&lt;![CDATA[$$\frac{2-1}{3} + \frac{2}{3}\cdot\frac{3}{14}$$]]&gt;&lt;/text&gt;
                  &lt;explanation&gt;&lt;![CDATA[This is the correct answer.]]&gt;&lt;/explanation&gt;
                &lt;/option&gt;
                &lt;option id=&quot;70b21dcaca416eede6f5052ce5ae7140&quot; selected_score=&quot;0&quot; unselected_score=&quot;0&quot;&gt;
                  &lt;text&gt;&lt;![CDATA[$$\frac{1-1}{3} + \frac{2}{3}\cdot\frac{3}{14}$$]]&gt;&lt;/text&gt;
                  &lt;explanation&gt;&lt;![CDATA[&quot;Sam&quot; occurs two times in this corpus, so the $$\frac{1-1}{3}$$ term is incorrect.]]&gt;&lt;/explanation&gt;
                &lt;/option&gt;
                &lt;option id=&quot;a705af3b168016fcfa9e6405bb6844a1&quot; selected_score=&quot;0&quot; unselected_score=&quot;0&quot;&gt;
                  &lt;text&gt;&lt;![CDATA[$$\frac{2-1}{3} + \frac{2}{3}\cdot\frac{3}{21}$$]]&gt;&lt;/text&gt;
                  &lt;explanation&gt;&lt;![CDATA[The $$|\{(w_{i-1}, w_i) : c(w_{i-1},w_i) &gt; 0\}|$$ computes the number of unique bigrams in the corpus, not the total number of bigrams.]]&gt;&lt;/explanation&gt;
                &lt;/option&gt;
                &lt;option id=&quot;f68af46c0f1eed91a5a95defe32fee9c&quot; selected_score=&quot;0&quot; unselected_score=&quot;0&quot;&gt;
                  &lt;text&gt;&lt;![CDATA[$$\frac{2-1}{3} + \frac{2}{3}\cdot\frac{4}{21}$$]]&gt;&lt;/text&gt;
                  &lt;explanation&gt;&lt;![CDATA[&quot;Sam&quot; has a continuation count of 3, not 4, since &quot;am Sam&quot; occurs twice.]]&gt;&lt;/explanation&gt;
                &lt;/option&gt;
              &lt;/option_group&gt;
            &lt;/option_groups&gt;
          &lt;/data&gt;
        &lt;/question&gt;
      &lt;/question_group&gt;
      &lt;question_group select=&quot;1&quot;&gt;
        &lt;preamble&gt;&lt;![CDATA[]]&gt;&lt;/preamble&gt;
        &lt;question id=&quot;56d548937aa992d8349a4f2510e01141&quot; type=&quot;GS_Choice_Answer_Question&quot;&gt;
          &lt;metadata&gt;
            &lt;parameters&gt;
              &lt;rescale_score&gt;1&lt;/rescale_score&gt;
              &lt;choice_type&gt;radio&lt;/choice_type&gt;
            &lt;/parameters&gt;
          &lt;/metadata&gt;
          &lt;data&gt;
            &lt;text&gt;&lt;![CDATA[We are given the following corpus, similar to the one in lecture but with &quot;ham&quot; replaced by &quot;Sam&quot; and &quot;I am Sam&quot; included twice:&lt;br&gt;

            &lt;ul&gt;
            &lt;li&gt;&amp;lt;s&amp;gt; I am Sam &amp;lt;/s&amp;gt;&lt;br&gt;&lt;/li&gt;
            &lt;li&gt;&amp;lt;s&amp;gt; Sam I am &amp;lt;/s&amp;gt;&lt;br&gt;&lt;/li&gt;
            &lt;li&gt;&amp;lt;s&amp;gt; I am Sam &amp;lt;/s&amp;gt;&lt;br&gt;&lt;/li&gt;
            &lt;li&gt;&amp;lt;s&amp;gt; I do not like green eggs and Sam &amp;lt;/s&amp;gt;&lt;/li&gt;
&lt;/ul&gt;

If we use linear interpolation smoothing between a maximum-likelihood bigram model and a maximum-likelihood unigram model with $$\lambda_1 = \frac{1}{2}$$ and $$\lambda_2 = \frac{1}{2}$$, what is $$P(\text{Sam} | \text{am})$$? Include &amp;lt;s&amp;gt; and  &amp;lt;/s&amp;gt; in your counts just like any other token.]]&gt;&lt;/text&gt;
            &lt;explanation&gt;&lt;![CDATA[$$\frac{1}{2} P(\text{Sam}) + \frac{1}{2} P(\text{Sam}|\text{am})= \frac{1}{2} \frac{C(\text{Sam})}{\sum_{w \in V} C(w)} +  \frac{1}{2} \frac{C(\text{am, Sam})}{C(\text{am})} = \frac{1}{2} \cdot \frac{4}{25} + \frac{1}{2} \cdot \frac{2}{3}$$.]]&gt;&lt;/explanation&gt;
            &lt;option_groups randomize=&quot;true&quot;&gt;
              &lt;option_group select=&quot;all&quot;&gt;
                &lt;option id=&quot;0c7c29211148af2552fe12f340342864&quot; selected_score=&quot;1&quot; unselected_score=&quot;0&quot;&gt;
                  &lt;text&gt;&lt;![CDATA[$$\frac{1}{2} \cdot \frac{4}{25} + \frac{1}{2} \cdot \frac{2}{3}$$]]&gt;&lt;/text&gt;
                  &lt;explanation&gt;&lt;![CDATA[This is the correct answer.]]&gt;&lt;/explanation&gt;
                &lt;/option&gt;
                &lt;option id=&quot;fb7f5d4e019d05812c76e3087f2e43d3&quot; selected_score=&quot;0&quot; unselected_score=&quot;0&quot;&gt;
                  &lt;text&gt;&lt;![CDATA[$$\frac{4}{25}$$]]&gt;&lt;/text&gt;
                  &lt;explanation&gt;&lt;![CDATA[This is $$P(\text{Sam} | \text{am})$$ using a maximum likelihood bigram model.]]&gt;&lt;/explanation&gt;
                &lt;/option&gt;
                &lt;option id=&quot;f8fab62ad3808c46145ec17895a46fcb&quot; selected_score=&quot;0&quot; unselected_score=&quot;0&quot;&gt;
                  &lt;text&gt;&lt;![CDATA[$$\frac{1}{2} \cdot \frac{4}{17} + \frac{1}{2} \cdot \frac{2}{3}$$]]&gt;&lt;/text&gt;
                  &lt;explanation&gt;&lt;![CDATA[We include the start and end of sentence markers in our counts, so there are more than 17 tokens.]]&gt;&lt;/explanation&gt;
                &lt;/option&gt;
                &lt;option id=&quot;128ee2caf25b721946c692aa099cb9ca&quot; selected_score=&quot;0&quot; unselected_score=&quot;0&quot;&gt;
                  &lt;text&gt;&lt;![CDATA[$$\frac{1}{2} \cdot \frac{4}{25} + \frac{1}{2} \cdot \frac{2}{2}$$]]&gt;&lt;/text&gt;
                  &lt;explanation&gt;&lt;![CDATA[&quot;Am&quot; occurs three times, so the last denominator is incorrect.]]&gt;&lt;/explanation&gt;
                &lt;/option&gt;
              &lt;/option_group&gt;
            &lt;/option_groups&gt;
          &lt;/data&gt;
        &lt;/question&gt;
      &lt;/question_group&gt;
      &lt;question_group select=&quot;1&quot;&gt;
        &lt;preamble&gt;&lt;![CDATA[]]&gt;&lt;/preamble&gt;
        &lt;question id=&quot;cefd17e5b405efb33919e8536a0f055a&quot; type=&quot;GS_Choice_Answer_Question&quot;&gt;
          &lt;metadata&gt;
            &lt;parameters&gt;
              &lt;rescale_score&gt;1&lt;/rescale_score&gt;
              &lt;choice_type&gt;radio&lt;/choice_type&gt;
            &lt;/parameters&gt;
          &lt;/metadata&gt;
          &lt;data&gt;
            &lt;text&gt;&lt;![CDATA[Suppose we train a bigram language model with add-one smoothing on a given corpus. The corpus contains $$V$$ word types. What is $$P(w_2|w_1)$$, where $$w_2$$ is a word which follows $$w_1$$? We use the notation $$c(w_1, w_2)$$ to denote the number of times that bigram $$(w_1, w_2)$$ occurs in the corpus, and $$c(w_i)$$ is the number of times word $$w_i$$ occurs.]]&gt;&lt;/text&gt;
            &lt;explanation&gt;&lt;![CDATA[Although there are $$V^2$$ possible bigrams in the corpus, we are only interested in bigrams which start with $$w_1$$. Thus, we add one to each of the $$V$$ values for $$P(w_2|w_1)$$, meaning we add $$V$$ to the denominator.]]&gt;&lt;/explanation&gt;
            &lt;option_groups randomize=&quot;true&quot;&gt;
              &lt;option_group select=&quot;all&quot;&gt;
                &lt;option id=&quot;3fb50629de69d2607113e3a2cc8e641f&quot; selected_score=&quot;0&quot; unselected_score=&quot;0&quot;&gt;
                  &lt;text&gt;&lt;![CDATA[$$\frac{c(w_1,w_2)}{c(w_1)}$$]]&gt;&lt;/text&gt;
                  &lt;explanation&gt;&lt;![CDATA[This is the maximum-likelihood estimate.]]&gt;&lt;/explanation&gt;
                &lt;/option&gt;
                &lt;option id=&quot;a33aa738db750ebe4e7fdffe44c42b10&quot; selected_score=&quot;1&quot; unselected_score=&quot;0&quot;&gt;
                  &lt;text&gt;&lt;![CDATA[$$\frac{c(w_1, w_2) + 1}{c(w_1) + V}$$]]&gt;&lt;/text&gt;
                  &lt;explanation&gt;&lt;![CDATA[This is the correct answer.]]&gt;&lt;/explanation&gt;
                &lt;/option&gt;
                &lt;option id=&quot;f19004ab7cb9d4e44398d37845ea52f5&quot; selected_score=&quot;0&quot; unselected_score=&quot;0&quot;&gt;
                  &lt;text&gt;&lt;![CDATA[$$\frac{c(w_1, w_2) + 1}{c(w_1) + V^2}$$]]&gt;&lt;/text&gt;
                  &lt;explanation&gt;&lt;![CDATA[Although there are $$V^2$$ possible bigrams in the corpus, we are only interested in bigrams which start with $$w_1$$.]]&gt;&lt;/explanation&gt;
                &lt;/option&gt;
                &lt;option id=&quot;64b53a5b1a58eed03b2d61c2582daca3&quot; selected_score=&quot;0&quot; unselected_score=&quot;0&quot;&gt;
                  &lt;text&gt;&lt;![CDATA[$$\frac{c(w_1, w_2) + V}{c(w_1) + V^2}$$]]&gt;&lt;/text&gt;
                  &lt;explanation&gt;&lt;![CDATA[We only add one to each term, not $$V$$.]]&gt;&lt;/explanation&gt;
                &lt;/option&gt;
              &lt;/option_group&gt;
            &lt;/option_groups&gt;
          &lt;/data&gt;
        &lt;/question&gt;
        &lt;question id=&quot;b87fa2a7ad3655daec97d1b226ec5908&quot; type=&quot;GS_Choice_Answer_Question&quot;&gt;
          &lt;metadata&gt;
            &lt;parameters&gt;
              &lt;rescale_score&gt;1&lt;/rescale_score&gt;
              &lt;choice_type&gt;radio&lt;/choice_type&gt;
            &lt;/parameters&gt;
          &lt;/metadata&gt;
          &lt;data&gt;
            &lt;text&gt;&lt;![CDATA[Suppose we train a trigram language model with add-one smoothing on a given corpus. The corpus contains $$V$$ word types. What is $$P(w_3|w_1, w_2)$$, where $$w_3$$ is a word which follows the bigram $$(w_1, w_2)$$? We use the notation $$c(w_1, w_2, w_3)$$ to denote the number of times that trigram $$(w_1, w_2, w_3)$$ occurs in the corpus, and so on for bigrams and unigrams.]]&gt;&lt;/text&gt;
            &lt;explanation&gt;&lt;![CDATA[Although there are $$V^3$$ possible trigrams in the corpus, we are only interested in bigrams which start with $$(w_1, w_2)$$. Thus, we add one to each of the $$V$$ values for $$P(w_3|w_1, w_2)$$, meaning we must add $$V$$ to the denominator.]]&gt;&lt;/explanation&gt;
            &lt;option_groups randomize=&quot;true&quot;&gt;
              &lt;option_group select=&quot;all&quot;&gt;
                &lt;option id=&quot;1dd1a64615fe2142de3eac4223605886&quot; selected_score=&quot;0&quot; unselected_score=&quot;0&quot;&gt;
                  &lt;text&gt;&lt;![CDATA[$$\frac{c(w_1,w_2, w_3)}{c(w_1, w_2)}$$]]&gt;&lt;/text&gt;
                  &lt;explanation&gt;&lt;![CDATA[This is the maximum-likelihood estimate.]]&gt;&lt;/explanation&gt;
                &lt;/option&gt;
                &lt;option id=&quot;1500d695fd6aad19c491c939bc85108a&quot; selected_score=&quot;1&quot; unselected_score=&quot;0&quot;&gt;
                  &lt;text&gt;&lt;![CDATA[$$\frac{c(w_1, w_2, w_3) + 1}{c(w_1, w_2) + V}$$]]&gt;&lt;/text&gt;
                  &lt;explanation&gt;&lt;![CDATA[This is the correct answer.]]&gt;&lt;/explanation&gt;
                &lt;/option&gt;
                &lt;option id=&quot;1ae5f5f27a5387ea48c236ca09ad1de3&quot; selected_score=&quot;0&quot; unselected_score=&quot;0&quot;&gt;
                  &lt;text&gt;&lt;![CDATA[$$\frac{c(w_1, w_2, w_3) + 1}{c(w_1, w_2) + V^3}$$]]&gt;&lt;/text&gt;
                  &lt;explanation&gt;&lt;![CDATA[Although there are $$V^3$$ possible trigrams in the corpus, we are only interested in trigrams which start with $$(w_1, w_2)$$.]]&gt;&lt;/explanation&gt;
                &lt;/option&gt;
                &lt;option id=&quot;a4cbc3ea10efbff2187f2d59c12147e6&quot; selected_score=&quot;0&quot; unselected_score=&quot;0&quot;&gt;
                  &lt;text&gt;&lt;![CDATA[$$\frac{c(w_1, w_2, w_3) + V}{c(w_1, w_2) + V^3}$$]]&gt;&lt;/text&gt;
                  &lt;explanation&gt;&lt;![CDATA[We add $$1$$ to each count, not $$V$$. The normalization is also incorrect.]]&gt;&lt;/explanation&gt;
                &lt;/option&gt;
              &lt;/option_group&gt;
            &lt;/option_groups&gt;
          &lt;/data&gt;
        &lt;/question&gt;
      &lt;/question_group&gt;
    &lt;/question_groups&gt;
  &lt;/data&gt;
&lt;/quiz&gt;
</textarea>
<p>
<input type="hidden" name="quiz_id" value="41">
<input type="hidden" name="quiz_type" value="quiz">
<input type="hidden" name="__csrf-token" value="Qwu8yFknj7oR4pIBHY9V">
<input type="submit" class="btn primary" name="submit" value="Save">
<input type="reset" class="btn info" name="reset" value="Reset" onclick="return confirm('All unsaved changes will be lost. Are you sure?')">
<a href="https://class.coursera.org/nlp-staging/admin/quiz/quiz_selector?quiz_type=quiz" onclick="return confirm('All unsaved changes will be lost. Are you sure?')" class="btn danger">Exit Without Saving</a>
</p>
<style>
  .CodeMirror{
    border: 1px solid #eee;
    
  }
  .CodeMirror-scroll{
    height: auto;
    overflow: visible;
  }
</style>
</form>
<script>$(document).ready(function(){
  CodeMirror.fromTextArea(document.getElementById('quiz_xml'),
  {lineNumbers: true, lineWrapping: true})
  })</script>        </div>
    </div>
            <form target="_blank" id="i18n_form" method="post" action="https://class.coursera.org/nlp-staging/admin/i18n/editor">
    	<span class="hide" id="i18n_span">{"0":"Coursera","1":"Preferences","2":"All Courses","3":"Admin","4":"Support","5":"I18N Editor","6":"About","7":"Contact Us","8":"Logout","9":"Natural Language Processing"}</span>
    	<input type="hidden" name="i18n_strings" id="i18n_strings" />
    </form>
        <div class="hidden">
        This page features MathJax technology to render mathematical formulae.
        If you are using a screen reader, please visit <a href="http://www.dessci.com/en/products/mathplayer/">MathPlayer</a> to download the plugin for your browser. Please note that this is an Internet Explorer-only plugin at this time.
    </div>
</body>

</html>
